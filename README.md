# Linglitter

Tools for building a bibliographic database of linguistics journal articles,
intended to support systematic literature reviews.

## The name

Linglitter takes the linguistics litter-ature (default: from 2005 through 2025) and turns it into compact nuggets of knowledge to inspire truly useful research ... or the destruction of linguistics as we know it.

## Contents

| File | Description |
|---|---|
| `scrape_dois.py` | Fetches DOIs and metadata from the CrossRef API for journals listed in `journals.json` |
| `scrape_pdfs.py` | Downloads open-access PDFs using the Unpaywall API |
| `scrape_repo.py` | Downloads PDFs from institutional repositories for non-OA articles |
| `scrape_openlibhum.py` | Crawls Open Library of Humanities journal websites to download PDFs |
| `prepare_manual.py` | Generates HTML list of articles requiring manual download |
| `integrate_manual.py` | Integrates manually downloaded PDFs (DOI-named) into data directory |
| `integrate_renaming.py` | Integrates manually downloaded PDFs (title-named) via fuzzy matching |
| `lookup_issns.py` | Interactive helper to look up ISSNs via CrossRef and add journals to `journals.json` |
| `journals.json` | Registry of target journals (name, publisher, ISSN) |
| `config.json` | Configuration for PDF scraping (year range, journals, politeness settings) |
| `linglitter.db` | SQLite database of scraped articles (generated by `scrape_dois.py`) |
| `bibliometrics/` | R scripts for data analysis and visualization |

## Requirements

### Python

- Python 3.8+
- `requests` (`pip install requests`)

### R (for bibliometrics)

- R 4.0+
- Packages: `DBI`, `RSQLite`, `dplyr`, `dbplyr`, `ggplot2`, `tidyr`, `scales`, `ggrepel`

```r
install.packages(c("DBI", "RSQLite", "dplyr", "dbplyr", "ggplot2", "tidyr", "scales", "ggrepel"))
```

## scrape_dois.py

Queries the [CrossRef REST API](https://api.crossref.org/) by journal ISSN and
publication-year range. Returns only items of type `journal-article`. Results
are stored in an SQLite database with DOI as primary key (re-runs are
idempotent).

### Usage

```bash
# All journals in journals.json, default years (2005--2025)
python scrape_dois.py

# Custom year range
python scrape_dois.py --from-year 2014 --until-year 2024

# Single publisher
python scrape_dois.py --publisher Benjamins

# Single journal
python scrape_dois.py --journal "Cognitive Linguistics"

# Use CrossRef polite pool for faster rate limits (recommended)
python scrape_dois.py --mailto you@example.com
```

### Options

| Flag | Default | Description |
|---|---|---|
| `--from-year` | `2005` | Start of year range, inclusive |
| `--until-year` | `2025` | End of year range, inclusive |
| `--journal` | all | Restrict to one journal (exact name from `journals.json`) |
| `--publisher` | all | Restrict to one publisher |
| `--db` | `linglitter.db` | Path to SQLite output |
| `--journals-file` | `journals.json` | Path to journal registry |
| `--mailto` | none | Email for CrossRef polite pool |

### Database schema

Table: `articles`

| Column | Type | Notes |
|---|---|---|
| `doi` | TEXT (PK) | e.g. `10.1515/cog-2022-0089` |
| `title` | TEXT | HTML tags stripped |
| `authors` | TEXT | `Family, Given; Family, Given; ...` |
| `journal` | TEXT | Full journal name from CrossRef |
| `year` | INTEGER | Publication year (prefers print over online date) |
| `volume` | TEXT | |
| `issue` | TEXT | |
| `pages` | TEXT | e.g. `261-296` |
| `publisher` | TEXT | From `journals.json` |
| `availability` | TEXT | `oa`, `no-oa`, `repo`, `manual`, or NULL (unknown) |
| `source` | TEXT | PDF URL from Unpaywall |
| `attempts` | INTEGER | Download attempt count (default 0) |
| `response` | INTEGER | HTTP status code (0 = no attempt) |
| `timestamp` | TEXT | ISO datetime of last attempt |
| `file` | TEXT | Relative path to downloaded PDF |

Indexed on `year` and `journal`.

## journals.json

To add a journal, append an entry:

```json
{
  "name": "Journal of Linguistics",
  "publisher": "Cambridge",
  "issn": ["0022-2267"]
}
```

ISSNs can be looked up at https://api.crossref.org/journals?query=JOURNAL+NAME.

## Current journal list

| Journal | Publisher |
|---|---|
| Cognitive Linguistics | De Gruyter |
| Cognitive Linguistic Studies | Benjamins |
| Constructions and Frames | Benjamins |
| Corpora | Edinburgh UP |
| Corpus Linguistics and Linguistic Theory | De Gruyter |
| International Journal of Corpus Linguistics | Benjamins |
| Language and Cognition | Cambridge |
| Morphology | Springer |
| Review of Cognitive Linguistics | Benjamins |
| The Mental Lexicon | Benjamins |
| Glossa | |
| Linguistic Inquiry | |
| Linguistic Analysis | |
| Theoretical Linguistics | |
| Natural Language and Linguistic Theory | |
| The Linguistic Review | |
| Journal of Memory and Language | |
| Mind and Language | |
| Language Sciences | |
| Language and Cognitive Processes | |
| Journal of Child Language | |
| Journal of Linguistics | |
| Linguistics Vanguard | |
| Languages | |
| Linguistics | |
| Lingua | |
| Language | |
| Written Language and Literacy | |
| Journal of Pragmatics | |
| Journal of Semantics | |
| Syntax and Semantics | |
| Language Variation and Change | |
| Register Studies | |
| Computational Linguistics | |
| Linguistics and Philosophy | |
| Natural Language Semantics | |
| Semantics and Pragmatics | |
| Syntax | |
| Proceedings of Sinn und Bedeutung | |
| Proceedings of the International Conference on Head-Driven Phrase Structure Grammar | |
| Cognition | |
| Language Learning | |
| Journal of Germanic Linguistics | |
| Zeitschrift für Sprachwissenschaft | |
| Zeitschrift für germanistische Linguistik | |
| Linguistische Berichte | |
| Germanistische Linguistik | |
| Zeitschrift für Dialektologie und Linguistik | |
| Journal of Language Modelling | Polish Academy of Sciences |
| Trends in Cognitive Sciences | Elsevier |
| Syntactic Theory and Research | Open Library of the Humanities |

## lookup_issns.py

Interactive helper that queries the CrossRef API for each journal name,
displays the top matches, and lets you pick the correct one (or enter an ISSN
manually). Confirmed entries are appended to `journals.json`.

```bash
# Standard interactive run
python lookup_issns.py

# With CrossRef polite pool
python lookup_issns.py --mailto you@example.com

# Preview without writing
python lookup_issns.py --dry-run
```

## scrape_pdfs.py

Downloads open-access PDFs using the [Unpaywall API](https://unpaywall.org/products/api).
Queries Unpaywall for each DOI in the database, downloads available PDFs, and
tracks download status.

### Usage

```bash
# Process one article (default)
python scrape_pdfs.py

# Process up to 100 articles
python scrape_pdfs.py --limit 100

# Run continuously until no candidates remain
python scrape_pdfs.py --continuous

# Preview without downloading
python scrape_pdfs.py --dry-run

# Use a different config file
python scrape_pdfs.py --config myconfig.json
```

### Options

| Flag | Default | Description |
|---|---|---|
| `--config` | `config.json` | Path to configuration file |
| `--db` | `linglitter.db` | Path to SQLite database |
| `--mailto` | none | Email for Unpaywall API (overrides config.json) |
| `--limit` | none | Maximum number of articles to process |
| `--continuous` | off | Run until no candidates remain |
| `--dry-run` | off | Show what would be done without downloading |

### How it works

1. Selects a random article from the database (respecting year range and journal list in config)
2. Skips articles already downloaded (`file IS NOT NULL`) or confirmed closed (`availability = 'no-oa'`)
3. Checks politeness intervals (global and per-publisher)
4. Queries Unpaywall API for the DOI
5. If OA: downloads PDF to `<data_dir>/<publisher>/<journal>/<year>/<doi>.pdf`
6. Updates database with availability status, source URL, attempt count, HTTP response, and file path

### Anti-scraping measures

The script uses browser-like headers, session cookies, and visits article landing
pages before downloading PDFs to work around publisher anti-scraping measures.
Downloaded files are verified using Content-Type headers and PDF magic bytes
(`%PDF-`) to detect when publishers serve HTML instead of PDF.

### Publisher compatibility

| Status | Publishers |
|---|---|
| Working | MDPI, OUP, PMC/NIH, De Gruyter, Benjamins, many others |
| HTML only | Wiley, Elsevier (some articles) — free HTML, paywalled PDF |
| Blocked | MIT Press (some subdomains) |

### Known limitations

- **HTML-only access**: Some publishers (Wiley, Elsevier) provide free HTML but
  require payment for PDF. These are correctly detected and skipped. Future
  enhancement: HTML-to-PDF conversion for text extraction/RAG use cases.
- **Aggressive anti-bot**: Some publishers may still block downloads. The script
  records failed attempts in the database for later retry or manual review.

## scrape_repo.py

Downloads PDFs from institutional repositories for articles marked as `no-oa`
in the database. Useful when you have institutional access to repositories that
provide PDFs not available via open access.

### Usage

```bash
# Process one article (default)
python scrape_repo.py

# Process up to 100 articles
python scrape_repo.py --limit 100

# Run continuously until no candidates remain
python scrape_repo.py --continuous

# Preview without downloading
python scrape_repo.py --dry-run
```

### Options

| Flag | Default | Description |
|---|---|---|
| `--config` | `config.json` | Path to configuration file |
| `--db` | `linglitter.db` | Path to SQLite database |
| `--limit` | none | Maximum number of articles to process |
| `--continuous` | off | Run until no candidates remain |
| `--dry-run` | off | Show what would be done without downloading |

### How it works

1. Selects a random article from the database where `availability = 'no-oa'` and `file IS NULL`
2. Fetches the landing page from each configured repository (`repo_url + DOI`)
3. Parses the HTML to find a download link (`<div class="download">` with `<a href>`)
4. If no download link found: sets `availability = 'manual'` (queued for manual download) and skips to next DOI with minimal delay
5. Downloads the PDF from the extracted link
6. Saves to `<data_dir>/<publisher>/<journal>/<year>/<doi>.pdf`
7. Updates database with `availability = 'repo'` on success

### Repository failure tracking

Each repository is allowed a configurable number of failed fetch attempts
(`max_repo_failures`, default 10). Once a repository exceeds this limit, it is
disabled for the current session. When all repositories are disabled, the script
exits with an error message.

### Configuration

Add the `local` section to `config.json`:

```json
{
  "local": {
    "repos": [
      "https://repo.example.edu/resolve/"
    ],
    "politeness_min": 180,
    "politeness_random": 20,
    "politeness_skip": 1,
    "max_repo_failures": 10
  }
}
```

| Field | Default | Description |
|---|---|---|
| `repos` | `[]` | List of repository URL prefixes (DOI is appended) |
| `politeness_min` | `180` | Minimum seconds between fetch attempts |
| `politeness_random` | `20` | Additional random delay (5 to this value) |
| `politeness_skip` | `1` | Seconds to wait when no download link found (no PDF downloaded) |
| `max_repo_failures` | `10` | Disable repo after this many failures |

## Manual Download Workflow

For articles that cannot be downloaded automatically (paywalled content requiring
manual browser interaction), three scripts provide a workflow:

1. `prepare_manual.py` — generates an HTML helper page for clicking through DOIs
2. `integrate_manual.py` — integrates PDFs named with encoded DOIs
3. `integrate_renaming.py` — integrates PDFs named with article titles (fuzzy matching)

### prepare_manual.py

Generates `manual.html` listing all articles marked for manual download
(`availability = 'manual'` and `file IS NULL`). The HTML page includes:

- Bibliographic citation for each article
- Clickable DOI link (opens in new tab)
- Encoded filename for saving the PDF
- Status tracking (red/green dots) with localStorage persistence
- Auto-copy of filename to clipboard when clicking a DOI link

```bash
python prepare_manual.py
python prepare_manual.py --output downloads.html
```

### integrate_manual.py

Integrates PDFs from the `manual_dir` directory (default: `manual/`). Files must
be named with the DOI encoded for filesystem safety (same encoding as other scripts:
`/ \ : * ? " < > | .` replaced with `_`), e.g., `10_1016_j_cognition_2022_105232.pdf`.

```bash
# Preview what would be done
python integrate_manual.py --dry-run

# Move files and update database
python integrate_manual.py
```

The script:
1. Matches each `{encoded_doi}.pdf` to a database entry
2. Moves matched files to `data/{publisher}/{journal}/{year}/{encoded_doi}.pdf`
3. Updates database: `source` = DOI URL, `file` = relative path
4. Warns and leaves unmatched files in place

### integrate_renaming.py

Integrates PDFs from subdirectories of `renaming_dir` (default: `renaming/`).
Subdirectory names must match journal names in the database exactly. Files are
named with article titles (or title prefixes).

```bash
python integrate_renaming.py
python integrate_renaming.py --threshold 70  # stricter matching
```

The script:
1. Scans each journal subdirectory for PDF files
2. Fuzzy-matches filenames against article titles in the database
3. Presents best match with score (0-100) and asks for confirmation:
   - **[Y]es** — rename to encoded DOI and move to data directory
   - **[D]elete** — delete the file
   - **[R]etain** — keep the file for manual handling
4. If no match found, offers Delete or Retain options

For better fuzzy matching, install `rapidfuzz`: `pip install --user rapidfuzz`

## scrape_openlibhum.py

Downloads PDFs from Open Library of Humanities journal websites (Glossa, STAR).
These journals are not indexed by Unpaywall, so this script crawls their websites directly.

### Usage

```bash
# Process Glossa (default)
python scrape_openlibhum.py

# Process STAR journal
python scrape_openlibhum.py --journal star

# Process up to 100 articles
python scrape_openlibhum.py --limit 100

# Preview without downloading
python scrape_openlibhum.py --dry-run
```

### Options

| Flag | Default | Description |
|---|---|---|
| `--config` | `config.json` | Path to configuration file |
| `--db` | `linglitter.db` | Path to SQLite database |
| `--journal` | `glossa` | Journal to crawl (`glossa` or `star`) |
| `--limit` | none | Maximum number of PDFs to download |
| `--dry-run` | off | Show what would be done without downloading |

### How it works

1. Starts from a configurable URL (e.g., `https://www.glossa-journal.org/issues/`)
2. Crawls internal links depth-first, staying on the journal domain
3. When reaching an article page (`/article/id/...`):
   - Extracts DOI from `<input id="share-link">` value attribute
   - Extracts PDF link from `<a href>Download PDF</a>`
   - Checks if DOI exists in database with matching journal and `file IS NULL`
   - Downloads PDF and updates database with `availability = 'oa'`
4. Respects politeness delay between all page fetches

### Configuration

Add journal sections to `config.json`:

```json
{
  "glossa": {
    "start_url": "https://www.glossa-journal.org/issues/",
    "domain": "www.glossa-journal.org",
    "article_prefix": "https://www.glossa-journal.org/article/id/",
    "db_journal": "Glossa: a journal of general linguistics",
    "politeness": 15
  },
  "star": {
    "start_url": "https://star-linguistics.org/issues/",
    "domain": "star-linguistics.org",
    "article_prefix": "https://star-linguistics.org/article/id/",
    "db_journal": "Syntactic Theory and Research",
    "politeness": 15
  }
}
```

| Field | Default | Description |
|---|---|---|
| `start_url` | — | URL to start crawling from |
| `domain` | — | Domain to stay within during crawl |
| `article_prefix` | — | URL prefix for article pages |
| `db_journal` | — | Journal name as stored in database |
| `politeness` | `15` | Seconds between page fetches |

## config.json

Configuration file for PDF scraping and integration scripts.

```json
{
  "years": [2020, 2025],
  "journals": ["Journal Name 1", "Journal Name 2"],
  "data_dir": "data",
  "manual_dir": "manual",
  "renaming_dir": "renaming",
  "unpaywall": {
    "mailto": "you@example.com",
    "politeness_interval": 1.0,
    "publisher_interval": 5.0,
    "max_attempts": 3
  },
  "local": {
    "repos": ["https://repo.example.edu/resolve/"],
    "politeness_min": 180,
    "politeness_random": 20,
    "politeness_skip": 1,
    "max_repo_failures": 10
  },
  "glossa": {
    "start_url": "https://www.glossa-journal.org/issues/",
    "politeness": 15
  }
}
```

| Field | Description |
|---|---|
| `years` | `[start, end]` year range (inclusive) |
| `journals` | List of journal names to process |
| `data_dir` | Directory for downloaded PDFs (default: `data`) |
| `manual_dir` | Directory for manually downloaded PDFs with DOI filenames (default: `manual`) |
| `renaming_dir` | Directory for manually downloaded PDFs with title filenames (default: `renaming`) |
| `unpaywall.mailto` | Email for Unpaywall API (or use `--mailto` flag) |
| `unpaywall.politeness_interval` | Seconds between any two download attempts |
| `unpaywall.publisher_interval` | Seconds between attempts from the same publisher |
| `unpaywall.max_attempts` | Give up after this many failed attempts per article |
| `local.repos` | List of repository URL prefixes for `scrape_repo.py` |
| `local.politeness_min` | Minimum seconds between repository fetch attempts |
| `local.politeness_random` | Additional random delay (5 to this value) |
| `local.politeness_skip` | Seconds to wait when no download link found (default 1) |
| `local.max_repo_failures` | Disable repository after this many failures |
| `glossa.start_url` | URL to start crawling for Glossa PDFs |
| `glossa.politeness` | Seconds between Glossa page fetches (default 15) |

## Bibliometrics (R scripts)

R scripts for analyzing the article database are in the `bibliometrics/` directory.

### publications_by_year.R

Generates line plots showing publications per journal over time.

```bash
cd bibliometrics
Rscript publications_by_year.R
```

Outputs:
- `publications_by_year_base.pdf` — Base R graphics line plot
- `publications_by_year_ggplot.pdf/.png` — ggplot2 line plot
- `publications_stacked_area.pdf/.png` — Stacked area chart (total + journal breakdown)
- `publications_stacked_proportional.pdf/.png` — 100% stacked area (journal share over time)
- `publications_by_year_faceted.pdf` — Faceted plot (one panel per journal)

## Next steps

- Additional journals / publishers as needed
- Additional PDF retrieval methods (e.g., institutional access, author repositories)

## Future steps

- Analyse citation structure
- Find most frequently cited monographs, make new database of these
- Download them also
- Some OA LangSci Press series to download and index in any case:
  
  - https://langsci-press.org/catalog/series/tbls
  - https://langsci-press.org/catalog/series/ogl
  - https://langsci-press.org/catalog/series/eotms
  - https://langsci-press.org/catalog/series/ogs
  - https://langsci-press.org/catalog/series/cogl
  - https://langsci-press.org/catalog/series/lv
  - https://langsci-press.org/catalog/series/cfls
  - https://langsci-press.org/catalog/series/frats
